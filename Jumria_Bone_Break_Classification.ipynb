{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7394372,
          "sourceType": "datasetVersion",
          "datasetId": 4257126
        }
      ],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Bone Break Classification using PyTorch and Timm",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyrch/DeepLearning-Tugas3/blob/main/Jumria_Bone_Break_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'bone-break-classification-image-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4257126%2F7394372%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240428%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240428T010849Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D63851e5556d2ef92990fc12d5e0cd4c36faae25fc5a1b1326a7509012946f0845f155beed82358554672dc18ca5116daef3c2cbc35350ee732c6ba75fe734d0fff746574b49a665f6ea43a13428466b346a620d81f896643b5bed822ea3b0803084475adc35efda29b7b97a45cad1b92b435dfb170b340c8eb68148b84a17884ad380da376cc4fd384ba22ebdc8b7f5f99adcfefc8734248ba5ce30f9c1a4fcf457d61b0af813e174c8df47b5d0189b99d2cf48331a45e47d409ab58ef820e5f681896fae303e1afbcc540ff57c00d604e7283fb9732101a6e24b3a5155b0ce1f6fd12eb51e0e2132c22172b92b7c4e25672f65bd9be02bd99718d53431edc21'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "41sUbc7q5-Fm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, shutil, numpy as np\n",
        "from glob import glob; from PIL import Image\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "torch.manual_seed(2023)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root, data, transformations = None):\n",
        "\n",
        "        self.transformations, self.data = transformations, data\n",
        "        self.im_paths = [im_path for im_path in sorted(glob(f\"{root}/*/{data}/*.jpg\"))]\n",
        "\n",
        "        self.cls_names, self.cls_counts, count, data_count = {}, {}, 0, 0\n",
        "        for idx, im_path in enumerate(self.im_paths):\n",
        "            class_name = self.get_class(im_path)\n",
        "            if class_name not in self.cls_names: self.cls_names[class_name] = count; self.cls_counts[class_name] = 1; count += 1\n",
        "            else: self.cls_counts[class_name] += 1\n",
        "\n",
        "    def get_class(self, path): return os.path.dirname(path).split(\"/\")[-2]\n",
        "\n",
        "    def __len__(self): return len(self.im_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        im_path = self.im_paths[idx]\n",
        "        im = Image.open(im_path).convert(\"RGB\")\n",
        "        gt = self.cls_names[self.get_class(im_path)]\n",
        "\n",
        "        if self.transformations is not None: im = self.transformations(im)\n",
        "\n",
        "        return im, gt\n",
        "\n",
        "def get_dls(root, transformations, bs, ns = 4):\n",
        "\n",
        "    ds = CustomDataset(root = root, data = \"Train\", transformations = transformations)\n",
        "    ts_ds = CustomDataset(root = root, data = \"Test\", transformations = transformations)\n",
        "\n",
        "    tr_len = int(len(ds) * 0.8); val_len = len(ds) - tr_len\n",
        "\n",
        "    tr_ds, vl_ds = random_split(dataset = ds, lengths = [tr_len, val_len])\n",
        "\n",
        "    tr_dl, val_dl, ts_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, num_workers = ns), DataLoader(vl_ds, batch_size = bs, shuffle = False, num_workers = ns), DataLoader(ts_ds, batch_size = 1, shuffle = False, num_workers = ns)\n",
        "\n",
        "    return tr_dl, val_dl, ts_dl, ds.cls_names\n",
        "\n",
        "root = \"/kaggle/input/bone-break-classification-image-dataset/Bone Break Classification/Bone Break Classification\"\n",
        "mean, std, im_size = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 224\n",
        "tfs = T.Compose([T.Resize((im_size, im_size)), T.ToTensor(), T.Normalize(mean = mean, std = std)])\n",
        "tr_dl, val_dl, ts_dl, classes = get_dls(root = root, transformations = tfs, bs = 16)\n",
        "\n",
        "print(len(tr_dl)); print(len(val_dl)); print(len(ts_dl)); print(classes)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:03.571297Z",
          "iopub.execute_input": "2024-01-13T11:38:03.571601Z",
          "iopub.status.idle": "2024-01-13T11:38:07.74675Z",
          "shell.execute_reply.started": "2024-01-13T11:38:03.571574Z",
          "shell.execute_reply": "2024-01-13T11:38:07.745619Z"
        },
        "trusted": true,
        "id": "-sFTEKmi5-Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def tensor_2_im(t, t_type = \"rgb\"):\n",
        "\n",
        "    gray_tfs = T.Compose([T.Normalize(mean = [ 0.], std = [1/0.5]), T.Normalize(mean = [-0.5], std = [1])])\n",
        "    rgb_tfs = T.Compose([T.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]), T.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ])])\n",
        "\n",
        "    invTrans = gray_tfs if t_type == \"gray\" else rgb_tfs\n",
        "\n",
        "    return (invTrans(t) * 255).detach().squeeze().cpu().permute(1,2,0).numpy().astype(np.uint8) if t_type == \"gray\" else (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8)\n",
        "\n",
        "def visualize(data, n_ims, rows, cmap = None, cls_names = None):\n",
        "\n",
        "    assert cmap in [\"rgb\", \"gray\"], \"Rasmni oq-qora yoki rangli ekanini aniqlashtirib bering!\"\n",
        "    if cmap == \"rgb\": cmap = \"viridis\"\n",
        "\n",
        "    plt.figure(figsize = (20, 10))\n",
        "    indekslar = [random.randint(0, len(data) - 1) for _ in range(n_ims)]\n",
        "    for idx, indeks in enumerate(indekslar):\n",
        "\n",
        "        im, gt = data[indeks]\n",
        "        # Start plot\n",
        "        plt.subplot(rows, n_ims // rows, idx + 1)\n",
        "        if cmap: plt.imshow(tensor_2_im(im, cmap), cmap=cmap)\n",
        "        else: plt.imshow(tensor_2_im(im))\n",
        "        plt.axis('off')\n",
        "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(gt)]}\")\n",
        "        else: plt.title(f\"GT -> {gt}\")\n",
        "\n",
        "visualize(tr_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:07.74917Z",
          "iopub.execute_input": "2024-01-13T11:38:07.749584Z",
          "iopub.status.idle": "2024-01-13T11:38:09.94558Z",
          "shell.execute_reply.started": "2024-01-13T11:38:07.749557Z",
          "shell.execute_reply": "2024-01-13T11:38:09.94455Z"
        },
        "trusted": true,
        "id": "9BS1yeH-5-Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(val_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:09.94683Z",
          "iopub.execute_input": "2024-01-13T11:38:09.947155Z",
          "iopub.status.idle": "2024-01-13T11:38:12.067619Z",
          "shell.execute_reply.started": "2024-01-13T11:38:09.947129Z",
          "shell.execute_reply": "2024-01-13T11:38:12.066692Z"
        },
        "trusted": true,
        "id": "kHEKoHS05-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(ts_dl.dataset, 20, 4, \"rgb\", list(classes.keys()))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:12.070312Z",
          "iopub.execute_input": "2024-01-13T11:38:12.070712Z",
          "iopub.status.idle": "2024-01-13T11:38:14.287396Z",
          "shell.execute_reply.started": "2024-01-13T11:38:12.070641Z",
          "shell.execute_reply": "2024-01-13T11:38:14.28643Z"
        },
        "trusted": true,
        "id": "dyNuwJZM5-Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_analysis(root, transformations, data_type = \"Train\"):\n",
        "\n",
        "    ds = CustomDataset(root = root, data = data_type, transformations = transformations)\n",
        "    cls_counts, width, text_width = ds.cls_counts,  0.8, 0.05\n",
        "    text_height = 4 if data_type == \"train\" else 0.3\n",
        "    cls_names = list(cls_counts.keys()); counts = list(cls_counts.values())\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (20, 10))\n",
        "    indices = np.arange(len(counts))\n",
        "\n",
        "    ax.bar(indices, counts, width, color = \"firebrick\")\n",
        "    ax.set_xlabel(\"Class Names\", color = \"red\")\n",
        "    ax.set_xticklabels(cls_names, rotation = 60)\n",
        "    ax.set(xticks = indices, xticklabels = cls_names)\n",
        "    ax.set_ylabel(\"Data Counts\", color = \"red\")\n",
        "    ax.set_title(f\"{data_type.upper()} Dataset Class Imbalance Analysis\")\n",
        "\n",
        "    for i, v in enumerate(counts): ax.text(i - text_width, v + text_height, str(v), color = \"royalblue\")\n",
        "\n",
        "data_analysis(root = root, transformations = tfs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:14.288575Z",
          "iopub.execute_input": "2024-01-13T11:38:14.288892Z",
          "iopub.status.idle": "2024-01-13T11:38:14.752293Z",
          "shell.execute_reply.started": "2024-01-13T11:38:14.288842Z",
          "shell.execute_reply": "2024-01-13T11:38:14.751368Z"
        },
        "trusted": true,
        "id": "dgOH8tx75-Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_analysis(root = root, transformations = tfs, data_type = \"Test\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:14.753602Z",
          "iopub.execute_input": "2024-01-13T11:38:14.754295Z",
          "iopub.status.idle": "2024-01-13T11:38:15.134116Z",
          "shell.execute_reply.started": "2024-01-13T11:38:14.754256Z",
          "shell.execute_reply": "2024-01-13T11:38:15.13319Z"
        },
        "trusted": true,
        "id": "US2f12R45-Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from tqdm import tqdm\n",
        "m = timm.create_model(\"rexnet_150\", pretrained = True, num_classes = len(classes))\n",
        "def train_setup(m): return m.to(\"cuda\").eval(), 50, \"cuda\", torch.nn.CrossEntropyLoss(), torch.optim.Adam(params = m.parameters(), lr = 1e-4)\n",
        "def to_device(batch, device): return batch[0].to(device), batch[1].to(device)\n",
        "def get_metrics(model, ims, gts, loss_fn, epoch_loss, epoch_acc): preds = model(ims); loss = loss_fn(preds, gts); return loss, epoch_loss + (loss.item()), epoch_acc + (torch.argmax(preds, dim = 1) == gts).sum().item()\n",
        "\n",
        "m, epochs, device, loss_fn, optimizer = train_setup(m)\n",
        "\n",
        "save_prefix, save_dir = \"bone\", \"saved_models\"\n",
        "print(\"Start training...\")\n",
        "best_acc, best_loss, threshold, not_improved, patience = 0, float(\"inf\"), 0.01, 0, 10\n",
        "tr_losses, val_losses, tr_accs, val_accs = [], [], [], []\n",
        "\n",
        "best_loss = float(torch.inf)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_loss, epoch_acc = 0, 0\n",
        "    for idx, batch in tqdm(enumerate(tr_dl)):\n",
        "\n",
        "        ims, gts = to_device(batch, device)\n",
        "\n",
        "        loss, epoch_loss, epoch_acc = get_metrics(m, ims, gts, loss_fn, epoch_loss, epoch_acc)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "    tr_loss_to_track = epoch_loss / len(tr_dl)\n",
        "    tr_acc_to_track  = epoch_acc / len(tr_dl.dataset)\n",
        "    tr_losses.append(tr_loss_to_track); tr_accs.append(tr_acc_to_track)\n",
        "\n",
        "    print(f\"{epoch + 1}-epoch train process is completed!\")\n",
        "    print(f\"{epoch + 1}-epoch train loss          -> {tr_loss_to_track:.3f}\")\n",
        "    print(f\"{epoch + 1}-epoch train accuracy      -> {tr_acc_to_track:.3f}\")\n",
        "\n",
        "    m.eval()\n",
        "    with torch.no_grad():\n",
        "        val_epoch_loss, val_epoch_acc = 0, 0\n",
        "        for idx, batch in enumerate(val_dl):\n",
        "            ims, gts = batch\n",
        "            ims, gts = ims.to(device), gts.to(device)\n",
        "\n",
        "            preds = m(ims)\n",
        "            loss = loss_fn(preds, gts)\n",
        "            pred_cls = torch.argmax(preds.data, dim = 1)\n",
        "            val_epoch_acc += (pred_cls == gts).sum().item()\n",
        "            val_epoch_loss += loss.item()\n",
        "\n",
        "        val_loss_to_track = val_epoch_loss / len(val_dl)\n",
        "        val_acc_to_track  = val_epoch_acc / len(val_dl.dataset)\n",
        "        val_losses.append(val_loss_to_track); val_accs.append(val_acc_to_track)\n",
        "\n",
        "        print(f\"{epoch + 1}-epoch validation process is completed!\")\n",
        "        print(f\"{epoch + 1}-epoch validation loss     -> {val_loss_to_track:.3f}\")\n",
        "        print(f\"{epoch + 1}-epoch validation accuracy -> {val_acc_to_track:.3f}\")\n",
        "\n",
        "        if val_loss_to_track < (best_loss + threshold):\n",
        "            os.makedirs(save_dir, exist_ok = True)\n",
        "            best_loss = val_loss_to_track\n",
        "            torch.save(m.state_dict(), f\"{save_dir}/{save_prefix}_best_model.pth\")\n",
        "\n",
        "        else:\n",
        "            not_improved += 1\n",
        "            print(f\"Loss value did not decrease for {not_improved} epochs\")\n",
        "            if not_improved == patience:\n",
        "                print(f\"Stop training since loss value did not decrease for {patience} epochs.\")\n",
        "                break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:38:15.135536Z",
          "iopub.execute_input": "2024-01-13T11:38:15.135835Z",
          "iopub.status.idle": "2024-01-13T11:39:30.258741Z",
          "shell.execute_reply.started": "2024-01-13T11:38:15.13581Z",
          "shell.execute_reply": "2024-01-13T11:39:30.257466Z"
        },
        "trusted": true,
        "id": "ahzzvNjm5-Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_curves(tr_losses, val_losses, tr_accs, val_accs):\n",
        "\n",
        "    plt.figure(figsize = (10, 5))\n",
        "    plt.plot(tr_losses, label = \"Train Loss\", c = \"red\")\n",
        "    plt.plot(val_losses, label = \"Validation Loss\", c = \"blue\")\n",
        "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss Values\")\n",
        "    plt.xticks(ticks = np.arange(len(tr_losses)), labels = [i for i in range(1, len(tr_losses) + 1)])\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "    plt.figure(figsize = (10, 5))\n",
        "    plt.plot(tr_accs, label = \"Train Accuracy\", c = \"orangered\")\n",
        "    plt.plot(val_accs, label = \"Validation Accuracy\", c = \"darkgreen\")\n",
        "    plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss Accuracy Scores\")\n",
        "    plt.xticks(ticks = np.arange(len(tr_accs)), labels = [i for i in range(1, len(tr_accs) + 1)])\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "learning_curves(tr_losses, val_losses, tr_accs, val_accs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:39:30.260917Z",
          "iopub.execute_input": "2024-01-13T11:39:30.261312Z",
          "iopub.status.idle": "2024-01-13T11:39:30.85371Z",
          "shell.execute_reply.started": "2024-01-13T11:39:30.261279Z",
          "shell.execute_reply": "2024-01-13T11:39:30.852772Z"
        },
        "trusted": true,
        "id": "4gQGnP615-Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "class SaveFeatures():\n",
        "\n",
        "    \"\"\" Extract pretrained activations\"\"\"\n",
        "    features = None\n",
        "    def __init__(self, m):\n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = ((output.cpu()).data).numpy()\n",
        "    def remove(self): self.hook.remove()\n",
        "\n",
        "def getCAM(conv_fs, linear_weights, class_idx):\n",
        "\n",
        "    bs, chs, h, w = conv_fs.shape\n",
        "    cam = linear_weights[class_idx].dot(conv_fs[0,:, :, ].reshape((chs, h * w)))\n",
        "    cam = cam.reshape(h, w)\n",
        "\n",
        "    return (cam - np.min(cam)) / np.max(cam)\n",
        "\n",
        "def inference(model, device, test_dl, num_ims, row, final_conv, fc_params, cls_names = None):\n",
        "\n",
        "    weight, acc = np.squeeze(fc_params[0].cpu().data.numpy()), 0\n",
        "    activated_features = SaveFeatures(final_conv)\n",
        "    preds, images, lbls = [], [], []\n",
        "    for idx, batch in tqdm(enumerate(test_dl)):\n",
        "        im, gt = to_device(batch, device)\n",
        "        pred_class = torch.argmax(model(im), dim = 1)\n",
        "        acc += (pred_class == gt).sum().item()\n",
        "        images.append(im)\n",
        "        preds.append(pred_class.item())\n",
        "        lbls.append(gt.item())\n",
        "\n",
        "    print(f\"Accuracy of the model on the test data -> {(acc / len(test_dl.dataset)):.3f}\")\n",
        "\n",
        "    plt.figure(figsize = (20, 10))\n",
        "    indekslar = [random.randint(0, len(images) - 1) for _ in range(num_ims)]\n",
        "\n",
        "    for idx, indeks in enumerate(indekslar):\n",
        "\n",
        "        im = images[indeks].squeeze()\n",
        "        pred_idx = preds[indeks]\n",
        "        heatmap = getCAM(activated_features.features, weight, pred_idx)\n",
        "\n",
        "        # Start plot\n",
        "        plt.subplot(row, num_ims // row, idx + 1)\n",
        "        plt.imshow(tensor_2_im(im), cmap = \"gray\"); plt.axis(\"off\")\n",
        "        plt.imshow(cv2.resize(heatmap, (im_size, im_size), interpolation=cv2.INTER_LINEAR), alpha=0.4, cmap='jet'); plt.axis(\"off\")\n",
        "\n",
        "        if cls_names is not None: plt.title(f\"GT -> {cls_names[int(lbls[indeks])]} ; PRED -> {cls_names[int(preds[indeks])]}\", color=(\"green\" if {cls_names[int(lbls[indeks])]} == {cls_names[int(preds[indeks])]} else \"red\"))\n",
        "        else: plt.title(f\"GT -> {gt} ; PRED -> {pred}\")\n",
        "\n",
        "m.load_state_dict(torch.load(f\"{save_dir}/{save_prefix}_best_model.pth\"))\n",
        "m.eval()\n",
        "final_conv, fc_params = m.features[-1], list(m.head.fc.parameters())\n",
        "inference(model = m.to(device), device = device, test_dl = ts_dl, num_ims = 20, row = 4, cls_names = list(classes.keys()), final_conv = final_conv, fc_params = fc_params)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-13T11:39:30.854978Z",
          "iopub.execute_input": "2024-01-13T11:39:30.855269Z",
          "iopub.status.idle": "2024-01-13T11:39:37.001349Z",
          "shell.execute_reply.started": "2024-01-13T11:39:30.855243Z",
          "shell.execute_reply": "2024-01-13T11:39:37.000221Z"
        },
        "trusted": true,
        "id": "XRPOFMWv5-Fz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}